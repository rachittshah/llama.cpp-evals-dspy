# Overview
This script demonstrates how to use the llama.cpp model with the dspy library to evaluate the correctness and engagingness of answers generated by the model. The script leverages several features from dspy, such as dspy.Predict, dspy.Signature, dspy.context, and dspy.evaluate.Evaluate, to perform this evaluation.

## Prerequisites
llama.cpp: Install using Homebrew with an OpenAI-compatible server.
DSPy: Python library for model evaluation.
Rich: For displaying results as tables in the terminal.

## Installation
Install llama.cpp:

```bash
brew install ggerganov/ggerganov/llama.cpp
```
Run the llama.cpp server:
```bash
llama-server --hf-repo TheBloke/Mistral-7B-Instruct-v0.2-GGUF --model mistral-7b-instruct-v0.2.Q4_K_M.gguf --hf-file mistral-7b-instruct-v0.2.Q4_K_M.gguf
```

Install the required Python libraries:

```bash
pip install dspy rich
```

## Script Breakdown

Model Initialization
The script initializes the llama.cpp model using the dspy.OpenAI class. It assumes that the llama-server is running on localhost:8080.

Example Dataset
The script defines a set of example question-answer pairs that are used to evaluate the model's performance. The answers are known, allowing for an assessment of the model's correctness and engagingness.

Signature Definition
A dspy.Signature class named Assess is defined to describe the input and output fields required for evaluating the quality of the model's responses.

Evaluation Metric
The correct_engaging_metric function is defined to calculate a score based on the model's ability to generate correct and engaging responses. This score is used to evaluate the performance of the llama.cpp model.

Evaluation Execution
The script uses the dspy.evaluate.Evaluate class to run the evaluation on the example dataset. The evaluation score is calculated and displayed in a table format using the rich library.

Output
The script prints a table summarizing the number of examples evaluated and the evaluation score as a percentage.

Running the Script
Ensure the llama-server is running.
Execute the script:
```
python evaluate_llama_cpp.py
```
Example Output

┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓
┃ Number of Examples    ┃ Evalute Score (%)    ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩
│ 3                     │ 100.0                │
└───────────────────────┴──────────────────────┘
This output indicates that the model scored 100% on the example dataset.

Notes
The api_key parameter in the llama_cpp_model initialization is set to "none" as a placeholder since the model doesn't require an API key.
The stop parameter is used to specify the stop word for the mistral-7b-instruct-v0.2 model.
The script uses multithreading for evaluation, which can be configured using the num_threads parameter in the Evaluate class.
This README should provide clear guidance on setting up, running, and understanding the evaluation script.
